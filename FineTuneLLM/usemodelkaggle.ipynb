{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13724172,"sourceType":"datasetVersion","datasetId":8731789}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T20:35:29.902967Z","iopub.execute_input":"2025-11-18T20:35:29.903201Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:08:14.127791Z","iopub.execute_input":"2025-11-14T16:08:14.128134Z","iopub.status.idle":"2025-11-14T16:08:52.745505Z","shell.execute_reply.started":"2025-11-14T16:08:14.128104Z","shell.execute_reply":"2025-11-14T16:08:52.744880Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-14 16:08:20.079589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763136500.262752      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763136500.315676      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install -q peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:08:59.301360Z","iopub.execute_input":"2025-11-14T16:08:59.302167Z","iopub.status.idle":"2025-11-14T16:09:02.906465Z","shell.execute_reply.started":"2025-11-14T16:08:59.302135Z","shell.execute_reply":"2025-11-14T16:09:02.905339Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from peft import PeftModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:09:05.831319Z","iopub.execute_input":"2025-11-14T16:09:05.831673Z","iopub.status.idle":"2025-11-14T16:09:09.345017Z","shell.execute_reply.started":"2025-11-14T16:09:05.831633Z","shell.execute_reply":"2025-11-14T16:09:09.343889Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:09:10.914212Z","iopub.execute_input":"2025-11-14T16:09:10.915200Z","iopub.status.idle":"2025-11-14T16:09:17.696830Z","shell.execute_reply.started":"2025-11-14T16:09:10.915165Z","shell.execute_reply":"2025-11-14T16:09:17.695877Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================\n# 1. Set Kaggle dataset path\n# ============================================================\nADAPTER_DIR = \"/kaggle/input/my-finetuned-llm\"   # Replace with your MAK if needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:28:00.698208Z","iopub.execute_input":"2025-11-14T16:28:00.698523Z","iopub.status.idle":"2025-11-14T16:28:00.702277Z","shell.execute_reply.started":"2025-11-14T16:28:00.698501Z","shell.execute_reply":"2025-11-14T16:28:00.701691Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:28:03.563738Z","iopub.execute_input":"2025-11-14T16:28:03.564484Z","iopub.status.idle":"2025-11-14T16:28:03.567926Z","shell.execute_reply.started":"2025-11-14T16:28:03.564458Z","shell.execute_reply":"2025-11-14T16:28:03.567177Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ============================================================\n# 2. Load 4-bit base model\n# ============================================================\nBASE_ID = \"unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    BASE_ID,\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\nprint(\"=\" * 50)\nprint(\"Base model loaded\")\nprint(\"=\" * 50)\n\n# ============================================================\n# 3. Load LoRA adapter\n# ============================================================\nmodel = PeftModel.from_pretrained(model, ADAPTER_DIR)\nprint(\"=\" * 50)\nprint(\"LoRA loaded\")\nprint(\"=\" * 50)\n\n# ============================================================\n# 4. Switch to inference mode\n# ============================================================\nFastLanguageModel.for_inference(model)\nprint(\"=\" * 50)\nprint(\"Model ready for inference\")\nprint(\"=\" * 50)\n\nmodel.eval()\nprint(\"\\n\\n\")\nprint(\"=\" * 50)\nprint(\"Fine-tuned model is ready!\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:28:06.566032Z","iopub.execute_input":"2025-11-14T16:28:06.566724Z","iopub.status.idle":"2025-11-14T16:28:19.072128Z","shell.execute_reply.started":"2025-11-14T16:28:06.566694Z","shell.execute_reply":"2025-11-14T16:28:19.071261Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n==================================================\nBase model loaded\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nLoRA loaded\n==================================================\n==================================================\nModel ready for inference\n==================================================\n\n\n\n==================================================\nFine-tuned model is ready!\n==================================================\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0))\nprint(\"Allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\nprint(\"Reserved:\", torch.cuda.memory_reserved() / 1024**2, \"MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:28:40.597133Z","iopub.execute_input":"2025-11-14T16:28:40.597984Z","iopub.status.idle":"2025-11-14T16:28:40.603443Z","shell.execute_reply.started":"2025-11-14T16:28:40.597959Z","shell.execute_reply":"2025-11-14T16:28:40.602672Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU name: Tesla T4\nAllocated: 11855.986328125 MB\nReserved: 12032.0 MB\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport re\nfrom threading import Lock\n\n# -------------------------\n# YOUR MODEL + TOKENIZER\n# -------------------------\ntok = tokenizer\nmdl = model\nmdl.eval()\n\ngen_lock = Lock()\n\n# -------------------------\n# Format the model output\n# -------------------------\ndef format_output(raw):\n    sections = {\n        \"verdict\": r\"(?i)verdict[:\\-]\\s*(.*)\",\n        \"score\": r\"(?i)score[:\\-]\\s*([0-9\\.]+)\",\n        \"benefits\": r\"(?i)benefits?[:\\-](.*?)(?=risks?:|overall|conclusion|$)\",\n        \"risks\": r\"(?i)risks?[:\\-](.*?)(?=overall|conclusion|$)\",\n        \"overall\": r\"(?i)(overall|rationale|analysis)[:\\-](.*)\",\n    }\n\n    formatted = {}\n    for key, pattern in sections.items():\n        m = re.search(pattern, raw, re.S)\n        if m:\n            text = m.group(1).strip()\n        else:\n            text = \"\"\n        formatted[key] = text\n\n    return formatted\n\n\n# -------------------------\n# Evaluation Function\n# -------------------------\nhistory = []\n\ndef evaluate_story(story, context):\n    with gen_lock:\n        if not story or story.strip() == \"\":\n            return \"Please enter a story.\", 0, history\n\n        template = \"\"\"Below is an instruction that describes a task.\n\n### Instruction:\nEvaluate whether the following idea uplifts humanity. Provide:\n- Verdict (Yes/No or Positive/Negative)\n- Score from 1 to 5 (integer number)\n- Benefits\n- Risks\n- Overall rationale\nBe objective and specific.\n\n### Input:\n{story}\n\n### Additional Context:\n{context}\n\n### Response:\n\"\"\"\n\n        prompt = template.format(\n            story = story.strip(),\n            context = context.strip() if context else \"None\"\n        )\n\n        inputs = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n        outputs = mdl.generate(\n            **inputs,\n            max_new_tokens = 400,\n            temperature   = 0.4,\n            top_p         = 0.95,\n            do_sample     = True,\n        )\n\n        raw_text = tok.decode(outputs[0], skip_special_tokens=True)\n\n        # Clean prefix if exists\n        if \"### Response:\" in raw_text:\n            raw_text = raw_text.split(\"### Response:\")[1].strip()\n\n        parsed = format_output(raw_text)\n\n        # Score handling\n        try:\n            score = int(parsed[\"score\"])\n        except:\n            score = 0\n\n        # Save history\n        history.append({\n            \"story\": story,\n            \"score\": score,\n            \"verdict\": parsed[\"verdict\"],\n            \"output\": raw_text[:600] + (\"...\" if len(raw_text) > 600 else \"\")\n        })\n\n        display_text = f\"\"\"\n### Verdict\n{parsed[\"verdict\"]}\n\n### Score\n{parsed[\"score\"]}\n\n### Benefits\n{parsed[\"benefits\"]}\n\n### Risks\n{parsed[\"risks\"]}\n\n### Overall Rationale\n{parsed[\"overall\"]}\n\"\"\".strip()\n\n        return display_text, score, history\n\n\n# -------------------------\n# Gradio UI\n# -------------------------\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# ğŸŒ Humanity Uplift Evaluator â€” Kaggle Demo\")\n    gr.Markdown(\"Enter an idea / story. The model evaluates whether it uplifts humanity.\")\n\n    with gr.Row():\n        story = gr.Textbox(\n            label=\"Story / Idea\",\n            placeholder=\"Enter the narrative or idea here...\",\n            lines=8\n        )\n        context = gr.Textbox(\n            label=\"Additional Context (optional)\",\n            placeholder=\"Studio mission, values, situation...\",\n            lines=4\n        )\n\n    btn = gr.Button(\"Evaluate\", variant=\"primary\")\n\n    with gr.Row():\n        output = gr.Markdown(label=\"Model Output\")\n        score_bar = gr.Slider(label=\"Score ( to 5)\", minimum=1, maximum=5, value=1, interactive=False)\n\n    history_box = gr.JSON(label=\"History (previous evaluations)\")\n\n    btn.click(\n        evaluate_story,\n        inputs=[story, context],\n        outputs=[output, score_bar, history_box]\n    )\n\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:40:00.696236Z","iopub.execute_input":"2025-11-14T16:40:00.696952Z","iopub.status.idle":"2025-11-14T16:40:02.125629Z","shell.execute_reply.started":"2025-11-14T16:40:00.696926Z","shell.execute_reply":"2025-11-14T16:40:02.124886Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://9a48064cda7da857ee.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://9a48064cda7da857ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Make sure if Gradio is working as expected\n\n# After running Gradio, run:\nimport torch\n\nprint(\"GPU:\", torch.cuda.get_device_name(0))\nprint(\"Memory allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}